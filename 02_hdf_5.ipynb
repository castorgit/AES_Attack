{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477cc2f6-c3a6-402e-bb54-a9f4d6d73bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9f727e-14c6-4ace-9fd9-1e44178f6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert datasets to CSV\n",
    "def export_to_csv(hdf_file, path, csv_name):\n",
    "    # Access the dataset within the HDF5 file\n",
    "    data = hdf_file[path][:]\n",
    "    \n",
    "    # Check if the data is already 1-dimensional\n",
    "    if len(data.shape) == 1:\n",
    "        # Create a pandas DataFrame with one column\n",
    "        df = pd.DataFrame(data, columns=[path])\n",
    "    else:\n",
    "        # If data is multi-dimensional, treat each column as a separate feature\n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    print(f\"Exported {path} to {csv_name}\")\n",
    "\n",
    "# Your file path based on the provided information\n",
    "file_path = './data/AES_PTv2_Pinata.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd6a769-00da-47c7-9226-238dbb28207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Pinata/Unprotected/Profiling/Labels to unprotected_labels.csv\n",
      "Exported Pinata/Unprotected/Profiling/MetaData to unprotected_metadata.csv\n",
      "Exported Pinata/Unprotected/Profiling/Traces to unprotected_traces.csv\n",
      "Exported Pinata/MS1/Profiling/Labels to MS1_labels.csv\n",
      "Exported Pinata/MS1/Profiling/MetaData to MS1_metadata.csv\n",
      "Exported Pinata/MS1/Profiling/Traces to MS1_traces.csv\n"
     ]
    }
   ],
   "source": [
    "# Open the HDF5 file\n",
    "with h5py.File(file_path, 'r') as hdf:\n",
    "    # Specify the base path for the 'Unprotected' group\n",
    "    base_path = 'Pinata/Unprotected/Profiling/'\n",
    "    \n",
    "    # Convert 'Labels' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Labels', './data/unprotected_labels.csv')\n",
    "    \n",
    "    # Convert 'MetaData' to CSV\n",
    "    export_to_csv(hdf, base_path + 'MetaData', './data/unprotected_metadata.csv')\n",
    "    \n",
    "    # Convert 'Traces' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Traces', './data/unprotected_traces.csv')\n",
    "\n",
    "with h5py.File(file_path, 'r') as hdf:\n",
    "    # Specify the base path for the 'Unprotected' group\n",
    "    base_path = 'Pinata/MS1/Profiling/'\n",
    "    \n",
    "    # Convert 'Labels' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Labels', './data/MS1_labels.csv')\n",
    "    \n",
    "    # Convert 'MetaData' to CSV\n",
    "    export_to_csv(hdf, base_path + 'MetaData', './data/MS1_metadata.csv')\n",
    "    \n",
    "    # Convert 'Traces' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Traces', './data/MS1_traces.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7232958-f693-4cfe-8ad7-f3fee808172e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stop\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f6f6b-7b8a-4148-a355-79b187b9d3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031236d-200a-4f0e-b01b-08415db4c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert datasets to CSV\n",
    "def export_to_csv(hdf_file, path, csv_name):\n",
    "    # Access the dataset within the HDF5 file\n",
    "    data = hdf_file[path][:]\n",
    "    \n",
    "    # Check if the data is already 1-dimensional\n",
    "    if len(data.shape) == 1:\n",
    "        # Create a pandas DataFrame with one column\n",
    "        df = pd.DataFrame(data, columns=[path])\n",
    "    else:\n",
    "        # If data is multi-dimensional, treat each column as a separate feature\n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    print(f\"Exported {path} to {csv_name}\")\n",
    "\n",
    "# Your file path based on the provided information\n",
    "file_path = './data/AES_PTv2_Pinata.h5'\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(file_path, 'r') as hdf:\n",
    "    # Specify the base path for the 'Unprotected' group\n",
    "    base_path = './data/'\n",
    "    \n",
    "    # Convert 'Labels' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Labels', 'unprotected_labels.csv')\n",
    "    \n",
    "    # Convert 'MetaData' to CSV\n",
    "    export_to_csv(hdf, base_path + 'MetaData', 'unprotected_metadata.csv')\n",
    "    \n",
    "    # Convert 'Traces' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Traces', 'unprotected_traces.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06651315-0f68-4ac2-88da-2c4c8e486956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert datasets to CSV\n",
    "def export_to_csv(hdf_file, path, csv_name):\n",
    "    # Access the dataset within the HDF5 file\n",
    "    data = hdf_file[path][:]\n",
    "    \n",
    "    # Check if the data is already 1-dimensional\n",
    "    if len(data.shape) == 1:\n",
    "        # Create a pandas DataFrame with one column\n",
    "        df = pd.DataFrame(data, columns=[path])\n",
    "    else:\n",
    "        # If data is multi-dimensional, treat each column as a separate feature\n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    print(f\"Exported {path} to {csv_name}\")\n",
    "\n",
    "# Your file path based on the provided information\n",
    "file_path = 'C:\\\\Users\\\\keert\\\\Downloads\\\\AES_PTv2_Pinata\\\\AES_PTv2_Pinata.h5'\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(file_path, 'r') as hdf:\n",
    "    # Specify the base path for the 'Unprotected' group\n",
    "    base_path = 'Pinata/Unprotected/Profiling/'\n",
    "    \n",
    "    # Convert 'Labels' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Labels', 'unprotected_labels.csv')\n",
    "    \n",
    "    # Convert 'MetaData' to CSV\n",
    "    export_to_csv(hdf, base_path + 'MetaData', 'unprotected_metadata.csv')\n",
    "    \n",
    "    # Convert 'Traces' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Traces', 'unprotected_traces.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1caef75-5fe8-4db2-a1ab-e52642523c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert datasets to CSV\n",
    "def export_to_csv(hdf_file, path, csv_name):\n",
    "    # Access the dataset within the HDF5 file\n",
    "    data = hdf_file[path][:]\n",
    "    \n",
    "    # Check if the data is already 1-dimensional\n",
    "    if len(data.shape) == 1:\n",
    "        # Create a pandas DataFrame with one column\n",
    "        df = pd.DataFrame(data, columns=[path])\n",
    "    else:\n",
    "        # If data is multi-dimensional, treat each column as a separate feature\n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    print(f\"Exported {path} to {csv_name}\")\n",
    "\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\keert\\\\Downloads\\\\\\AES_PTv2_D1\\\\\\AES_PTv2_D1.h5'\n",
    "\n",
    "# Open the HDF5 file\n",
    "\n",
    "with h5py.File(file_path, 'r') as hdf:\n",
    "    # Specify the base path for the 'Unprotected' group\n",
    "    base_path = 'D1/MS2/Profiling/'\n",
    "    \n",
    "    # Convert 'Labels' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Labels', 'MS2_labels.csv')\n",
    "    \n",
    "    # Convert 'MetaData' to CSV\n",
    "    export_to_csv(hdf, base_path + 'MetaData', 'MS2_metadata.csv')\n",
    "    \n",
    "    # Convert 'Traces' to CSV\n",
    "    export_to_csv(hdf, base_path + 'Traces', 'MS2_traces.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25f1f1-e7d6-4715-96fe-29d8386ef416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the power traces from the provided CSV file\n",
    "power_traces_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/Unprotected/Profiling/unprotected_traces.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5400c14-9a3b-484d-b523-b195868f9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a subset of power traces for visual inspection\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):  # Adjust the range for the number of traces you want to plot\n",
    "    plt.plot(power_traces_df.iloc[i], label=f'Trace {i}')\n",
    "plt.title('Power Trace Samples')\n",
    "plt.xlabel('Sample Point Index')\n",
    "plt.ylabel('Power Measurement')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Calculate statistical metrics for the power traces\n",
    "stats = power_traces_df.describe()\n",
    "\n",
    "# Print the calculated statistics\n",
    "print(stats)\n",
    "\n",
    "# It might be useful to plot histograms of the power values to see their distribution\n",
    "power_traces_df.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385dd63e-1f6f-433c-b441-c6ff86bcdabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute basic statistics for each trace\n",
    "statistics = power_traces_df.describe()\n",
    "\n",
    "# Display statistics like mean, median, variance, standard deviation\n",
    "print(statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3910929-b1ef-4ddd-bbc3-3a666dd43e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "# Compute FFT for each trace\n",
    "fft_results = fft(power_traces_df.values, axis=1)\n",
    "\n",
    "# Plot the FFT results for a subset of traces\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.plot(abs(fft_results[i, :]), label=f'FFT Trace {i}')\n",
    "plt.title('FFT of Power Traces')\n",
    "plt.xlabel('Frequency Bin')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb39845-2698-4bb8-a647-f2e30ff4c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time-frequency analysis\n",
    "import numpy as np\n",
    "from scipy.signal import stft\n",
    "\n",
    "# Compute STFT for a single trace as an example\n",
    "frequencies, times, Zxx = stft(power_traces_df.values[0], fs=1.0) # fs can vary\n",
    "\n",
    "# Plot STFT result\n",
    "plt.pcolormesh(times, frequencies, np.abs(Zxx))\n",
    "plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389e9b3-ee03-4ae8-bcc4-1d6739ddbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Correlation of the first trace with the second\n",
    "correlation = np.corrcoef(power_traces_df.iloc[0], power_traces_df.iloc[1])\n",
    "print(f'Correlation coefficient: {correlation[0, 1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9828066-ae7a-4ae8-a66f-5ca490ad72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#Power trace Analysis using various methods\n",
    "#Finding basis stats:\n",
    "column_stats = power_traces_df.describe()\n",
    "print(column_stats)\n",
    "#Analyze power correlations\n",
    "column_correlations = power_traces_df.corr()\n",
    "print(column_correlations)\n",
    "#heatmap\n",
    "sns.heatmap(column_correlations, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Heatmap of Power Trace Correlations')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4fe97-46af-42c1-a40f-acc42098b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming power_traces_df is your DataFrame with traces as rows\n",
    "kmeans = KMeans(n_clusters=5)  # Choose an appropriate number of clusters\n",
    "clusters = kmeans.fit_predict(power_traces_df)\n",
    "\n",
    "# Plot the clusters for the first two dimensions as an example\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(power_traces_df.iloc[:, 0], power_traces_df.iloc[:, 1], c=clusters, cmap='viridis')\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.title('Cluster Analysis of Power Traces')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6ac0d-e850-47b2-a257-e91a960ad8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Conduct PCA\n",
    "pca = PCA(n_components=2)  # Adjust components for the level of variance you want to capture\n",
    "principal_components = pca.fit_transform(power_traces_df)\n",
    "\n",
    "# Convert to a DataFrame for ease of plotting\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Plot the first two principal components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Power Traces')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35fa0d-048d-41cf-9c0e-5698498aa8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporal patterns\n",
    "# Plot temporal patterns for the first trace as an example\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(power_traces_df.iloc[0, :])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Power Consumption')\n",
    "plt.title('Temporal Power Consumption Pattern for a Single Trace')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1846d30-0a42-4edf-bde5-f4a53852f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION USING RANDOM FOREST\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "# Load your datasets\n",
    "unprotected_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/PINATA/Unprotected/Profiling/unprotected_traces.csv')\n",
    "masked_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/PINATA/MASK1/Profiling/MS1_traces.csv')\n",
    "\n",
    "# Label & combining dataset\n",
    "unprotected_df['label'] = 0  # 0 for unprotected\n",
    "masked_df['label'] = 1       # 1 for masked\n",
    "combined_df = pd.concat([unprotected_df, masked_df])\n",
    "\n",
    "# Feature Engineering \n",
    "# features = combined_df.drop('label', axis=1).apply(lambda x: [x.mean(), x.std(), x.min(), x.max()], axis=1, result_type='expand')\n",
    "# features.columns = ['mean', 'std', 'min', 'max']\n",
    "# labels = combined_df['label']\n",
    "\n",
    "# NOTE: Using all the features rather than focusing on the 4 important features used: Darshana's suggestion\n",
    "X = combined_df.drop(columns=['label'])  # This now includes all original features without any aggregation\n",
    "y = combined_df['label']\n",
    "print(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model Training (Trying using 10 instead of 100 n-estimator): Darshana Suggestion\n",
    "classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model Prediction\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test_scaled)[:, 1])\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "# Feature Importance Analysis\n",
    "importances = classifier.feature_importances_\n",
    "feature_names = features.columns\n",
    "importances_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "print(importances_df)\n",
    "\n",
    "# Permutation Feature Importance\n",
    "perm_importance = permutation_importance(classifier, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'feature': feature_names, 'importance_mean': perm_importance.importances_mean}).sort_values('importance_mean', ascending=False)\n",
    "print(perm_importance_df)\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(classifier, X_train_scaled, y_train, cv=5)\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores}')\n",
    "print(f'Mean CV Accuracy: {np.mean(cv_scores)}')\n",
    "\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test_scaled)[:, 1])\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "# Model Evaluation\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test_scaled)[:, 1])\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "# Feature Importance Analysis\n",
    "importances = classifier.feature_importances_\n",
    "feature_names = features.columns\n",
    "importances_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "print(importances_df)\n",
    "\n",
    "# Permutation Feature Importance\n",
    "perm_importance = permutation_importance(classifier, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'feature': feature_names, 'importance_mean': perm_importance.importances_mean}).sort_values('importance_mean', ascending=False)\n",
    "print(perm_importance_df)\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(classifier, X_train_scaled, y_train, cv=5)\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores}')\n",
    "print(f'Mean CV Accuracy: {np.mean(cv_scores)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485bc6df-b28b-420b-aade-237b2b183bab",
   "metadata": {},
   "source": [
    "# BELOW CODE FAILED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3b579-d8fa-42d6-8eaf-8a937d2375c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing it with the data from the D1 set for both unprotected and asked1 implementation\n",
    "# Load new datasets for another device (replace the paths with your new dataset paths)\n",
    "new_unprotected_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/D1/Unprotected/Profiling/unprotected_traces.csv')\n",
    "new_masked_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/D1/MASK1/Profiling/MS1_traces.csv')\n",
    "\n",
    "# Add a 'label' column to each dataset\n",
    "new_unprotected_df['label'] = 0  # 0 for unprotected\n",
    "new_masked_df['label'] = 1       # 1 for masked\n",
    "\n",
    "# Combine new datasets\n",
    "new_combined_df = pd.concat([new_unprotected_df, new_masked_df])\n",
    "\n",
    "# Feature Engineering for the new dataset\n",
    "new_features = new_combined_df.drop('label', axis=1).apply(lambda x: [x.mean(), x.std(), x.min(), x.max()], axis=1, result_type='expand')\n",
    "new_features.columns = ['mean', 'std', 'min', 'max']\n",
    "new_labels = new_combined_df['label'].values\n",
    "\n",
    "# Scale the new features using the already fitted scaler\n",
    "new_features_scaled = scaler.transform(new_features)\n",
    "\n",
    "# Predict on the new device data using the trained classifier\n",
    "new_y_pred = classifier.predict(new_features_scaled)\n",
    "\n",
    "# Evaluate the model on the new device data\n",
    "print(\"Confusion Matrix for the new device data:\")\n",
    "print(confusion_matrix(new_labels, new_y_pred))\n",
    "print(\"\\nClassification Report for the new device data:\")\n",
    "print(classification_report(new_labels, new_y_pred))\n",
    "\n",
    "# If you're interested in the ROC-AUC score for the new dataset\n",
    "new_y_pred_proba = classifier.predict_proba(new_features_scaled)[:, 1]  # Probabilities for the positive class\n",
    "new_roc_auc = roc_auc_score(new_labels, new_y_pred_proba)\n",
    "print(f\"ROC-AUC Score for the new device data: {new_roc_auc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec626b4-4883-44a9-8a46-244970adb5b9",
   "metadata": {},
   "source": [
    "### Testing with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ef11b-71d9-4700-9c53-7a3e5c80b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking with unprotected data\n",
    "unseen_unprotected_df = pd.read_csv('C:/Users/keert/Downloads/test_data_unprotected.csv')\n",
    "unseen_features = unseen_unprotected_df.apply(lambda x: [x.mean(), x.std(), x.min(), x.max()], axis=1, result_type='expand')\n",
    "unseen_features.columns = ['mean', 'std', 'min', 'max']\n",
    "# Assuming 'scaler' is your StandardScaler instance\n",
    "unseen_features_scaled = scaler.transform(unseen_features)\n",
    "unseen_predictions = classifier.predict(unseen_features_scaled)\n",
    "unseen_probabilities = classifier.predict_proba(unseen_features_scaled)\n",
    "# This will give you the probability of each class per instance\n",
    "# Simple analysis assuming you expect all predictions to be '0' (unprotected)\n",
    "correct_predictions = (unseen_predictions == 0).sum()\n",
    "total_predictions = len(unseen_predictions)\n",
    "print(f\"Correct Unprotected Predictions: {correct_predictions} out of {total_predictions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7be1b-7f38-456d-944b-1de295914265",
   "metadata": {},
   "source": [
    "### wfi including normalization to the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f6890-4ec8-4059-8552-a0d65b864bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001bf204-1862-4d6b-b304-e4e9c4b323af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING NORMALIZATION, THEN WITHOUT SELECTING ANY PARTICULAR FEATURE AND USING ALL THE POSSIBLE FEATURES, WE HAVE USED RANDOM FOREST CLASSIFIER AND THEN FOUND THE\n",
    "#PARAMATER AND FEATURE IMPORTANCE RANKING AND FOUND CLASSIFICATION REPORT AND THE EVALL METRICS\n",
    "# K FOLD CROSS VALIDATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "# Correctly import SimpleImputer from sklearn.impute\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load datasets\n",
    "unprotected_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/PINATA/Unprotected/Profiling/unprotected_traces.csv')\n",
    "masked_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/PINATA/MASK1/Profiling/MS1_traces.csv')\n",
    "\n",
    "# Initialization of scalers and imputer\n",
    "robust_scaler = RobustScaler()\n",
    "min_max_scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer(strategy='median')  # Choosing 'median' strategy for imputation\n",
    "\n",
    "# Label & combine dataset\n",
    "unprotected_df['label'] = 0  # 0 for unprotected\n",
    "masked_df['label'] = 1       # 1 for masked\n",
    "combined_df = pd.concat([unprotected_df, masked_df])\n",
    "\n",
    "# Separate numerical features and impute missing values\n",
    "numerical_features = combined_df.drop(columns=['label']) \n",
    "imputed_features = imputer.fit_transform(numerical_features)  # Impute missing values\n",
    "\n",
    "# Normalization using RobustScaler followed by MinMaxScaler\n",
    "robust_scaled_features = robust_scaler.fit_transform(imputed_features)\n",
    "min_max_scaled_features = min_max_scaler.fit_transform(robust_scaled_features)\n",
    "\n",
    "# Update combined_df with scaled features\n",
    "scaled_df = pd.DataFrame(min_max_scaled_features, columns=numerical_features.columns, index=numerical_features.index)\n",
    "combined_df.update(scaled_df)\n",
    "\n",
    "# Prepare features and labels for model training\n",
    "X = scaled_df  # Using directly scaled_df as X\n",
    "y = combined_df['label']\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Model Prediction\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1])}')\n",
    "\n",
    "# Feature Importance\n",
    "feature_importances = classifier.feature_importances_\n",
    "features = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "print(\"Feature Importances:\\n\", feature_importance_df)\n",
    "\n",
    "# Permutation Feature Importance\n",
    "perm_importance = permutation_importance(classifier, X_test, y_test, n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': features, 'Importance Mean': perm_importance.importances_mean}).sort_values(by='Importance Mean', ascending=False)\n",
    "print(\"Permutation Feature Importances:\\n\", perm_importance_df)\n",
    "\n",
    "# Cross-Validation Accuracy Scores\n",
    "cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores.mean()}')\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Model Prediction\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1])}')\n",
    "\n",
    "# Feature Importance\n",
    "feature_importances = classifier.feature_importances_\n",
    "features = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "print(\"Feature Importances:\\n\", feature_importance_df)\n",
    "\n",
    "# Permutation Feature Importance\n",
    "perm_importance = permutation_importance(classifier, X_test, y_test, n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': features, 'Importance Mean': perm_importance.importances_mean}).sort_values(by='Importance Mean', ascending=False)\n",
    "print(\"Permutation Feature Importances:\\n\", perm_importance_df)\n",
    "\n",
    "# Cross-Validation Accuracy Scores\n",
    "cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores.mean()}')\n",
    "\n",
    "# Corrected Data Splitting using X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Continue with the RandomForestClassifier training and evaluation as before\n",
    "classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1])}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee66454-1d1b-4b0b-900e-09c20c9e28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION USING RANDOM FOREST\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load your datasets\n",
    "unprotected_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/PINATA/Unprotected/Profiling/unprotected_traces_v2')\n",
    "masked_df = pd.read_csv('C:/Users/keert/Downloads/CSV_Dataset/PINATA/MASK1/Profiling/MS1_traces.csv')\n",
    "\n",
    "# Label & combining dataset\n",
    "unprotected_df['label'] = 0  # 0 for unprotected\n",
    "masked_df['label'] = 1       # 1 for masked\n",
    "combined_df = pd.concat([unprotected_df, masked_df])\n",
    "\n",
    "# Using all the features rather than focusing on the 4 important features used\n",
    "X = combined_df.drop(columns=['label'])  # This now includes all original features without any aggregation\n",
    "y = combined_df['label']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model Training\n",
    "classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model Prediction\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test_scaled)[:, 1])\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "# Feature Importance Analysis\n",
    "feature_names = X.columns\n",
    "importances = classifier.feature_importances_\n",
    "importances_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
    "print(\"Feature Importances:\\n\", importances_df)\n",
    "\n",
    "# Permutation Feature Importance\n",
    "perm_importance = permutation_importance(classifier, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'feature': feature_names, 'importance_mean': perm_importance.importances_mean}).sort_values(by='importance_mean', ascending=False)\n",
    "print(\"Permutation Feature Importances:\\n\", perm_importance_df)\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(classifier, X_train_scaled, y_train, cv=5)\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores.mean()}')\n",
    "\n",
    "\n",
    "# Model Prediction\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test_scaled)[:, 1])\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "# Feature Importance Analysis\n",
    "importances = classifier.feature_importances_\n",
    "feature_names = features.columns\n",
    "importances_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "print(importances_df)\n",
    "\n",
    "# Permutation Feature Importance\n",
    "perm_importance = permutation_importance(classifier, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'feature': feature_names, 'importance_mean': perm_importance.importances_mean}).sort_values('importance_mean', ascending=False)\n",
    "print(perm_importance_df)\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(classifier, X_train_scaled, y_train, cv=5)\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores}')\n",
    "print(f'Mean CV Accuracy: {np.mean(cv_scores)}')\n",
    "\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test_scaled)[:, 1])\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "# Model Evaluation\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test_scaled)[:, 1])\n",
    "print(f'ROC-AUC Score: {roc_auc}')\n",
    "\n",
    "# Feature Importance Analysis\n",
    "importances = classifier.feature_importances_\n",
    "feature_names = features.columns\n",
    "importances_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "print(importances_df)\n",
    "\n",
    "# Permutation Feature Importance\n",
    "perm_importance = permutation_importance(classifier, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'feature': feature_names, 'importance_mean': perm_importance.importances_mean}).sort_values('importance_mean', ascending=False)\n",
    "print(perm_importance_df)\n",
    "\n",
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(classifier, X_train_scaled, y_train, cv=5)\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores}')\n",
    "print(f'Mean CV Accuracy: {np.mean(cv_scores)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38317feb-6d93-4f57-898d-846981449b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
